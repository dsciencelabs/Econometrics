--- 
title: "Econometrics"
author: "Bakti Siregar, M.Sc"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: dsciencelabs/Econometrics
description: "Deskripsi Singkat Tentang Buku"
tags: [algoritma, pemrograman, r-programming]
cover-image: images/EBook_Cover.png
---

```{r setup, include=FALSE}

options(

  htmltools.dir.version = FALSE, formatR.indent = 2,

  width = 55, digits = 4, warnPartialMatchAttr = FALSE, warnPartialMatchDollar = FALSE

)


lapply(c("rmarkdown","bookdown",
         "limSolve","rootSolve",
         "xts","numDeriv",
         "pracma","gaussquad",
         "deSolve","ReacTran",
         "lmtest", "car"), function(pkg) {

  if (system.file(package = pkg) == '') install.packages(pkg)

})

```

# Foreword{-}

Welcome to the exciting and complex world of econometrics! This book is designed to provide an in-depth understanding of the basic concepts and practical applications in econometric analysis. Econometrics is a branch of economics that combines economic theory with mathematical and statistical methods to test economic hypotheses and make predictions about economic phenomena in the real world.

This book was prepared with the aim of providing a comprehensive guide, starting from the basics of econometrics to advanced applications relevant to the contemporary world of economics. Readers will be guided through basic concepts such as linear regression, the assumptions underlying econometric models, to sophisticated data analysis techniques.

Emphasis is placed on the application of econometrics in the context of modern economics, with relevant case studies and applicable examples. The book also integrates the latest developments in the field of econometrics, ensuring that readers gain insight into the latest approaches and methodological developments.


```{r fig.align='center', echo=FALSE, include=identical(knitr:::pandoc_to(), 'html'), out.width='50%'}

knitr::include_graphics('images/EBook_Cover.png', dpi = NA)

```

As the author, I hope that this book will not only be a valuable reference source for students, academics, and economic practitioners, but will also inspire further interest and understanding of the complexity of economic phenomena. I would like to thank everyone who has contributed and supported me in the process of writing this book.

## Summary {-}

Econometrics is a branch of economics that uses mathematical and statistical methods to analyze and test hypotheses in an economic context. It combines economic concepts with statistical tools to understand and model economic relationships.

Here are some common topics in econometrics:

* **Regression Models:** Econometrics often begins with the study of regression models, in which the relationship between one or more dependent and independent variables is explained. Regression models can be simple or complex depending on the data and problem at hand.

* **Regression Assumptions:** Regression analysis requires several assumptions, such as the homoscedasticity assumption (constant residual variance), the residual independence assumption, and the residual normality assumption.

* **Parameter Estimation:** Using empirical data, we can estimate parameters in a regression model to find out how well the model fits the data.

* **Hypothesis Testing:** Econometrics is used to test statistical hypotheses about model parameters. This helps determine whether the observed relationship is significantly different from zero.

* **Multicollinearity and Heteroscedasticity:** Some common problems in regression analysis involve multicollinearity (high correlation between independent variables) and heteroscedasticity (non-constant residual variance). Econometricians develop methods to deal with these problems.

* **Time Series Model:** Econometrics is also often used to analyze time series data, which includes observations taken sequentially over a certain time.

* **Applied Econometrics:** In addition to basic models, econometrics is also applied in various fields of economics, including finance, human resources, marketing, and others.


## Writer {-}

<img src = "https://github.com/dsciencelabs/images/blob/master/Penulis/Bakti.jpg?raw=true" width = "150" height = "150" style = "float:right; margin-left: 10px; margin-top: 7px" />

- **[Bakti Siregar, M.Sc](https://www.linkedin.com/in/dsciencelabs/)** is Head of the Study Program at the Department of Statistics, Matana University. Graduate of Masters in Applied Mathematics from National Sun Yat Sen University, Taiwan. He is also a lecturer and Data Scientist consultant at well-known companies such as [JNE](https://www.jne.co.id/id/beranda), [Samora Group](https://www.samoragroup.co. id/home/en), [Pertamina](https://www.pertamina.com/), and others. He has special enthusiasm in teaching Big Data Analytics, Machine Learning, Optimization, and Time Series Analysis in the fields of finance and investment. His expertise is also evident in the use of Statistical programming languages such as R Studio and Python. He applies the MySQL/NoSQL database system in learning data management, and is proficient in using Big Data tools such as Spark and Hadoop. Some of his projects can be seen at the following links: [Rpubs](https://rpubs.com/dsciencelabs), [Github](https://github.com/dsciencelabs), [Website](https://dsciencelabs.github .io/web/index.html), and [Kaggle](https://www.kaggle.com/baktisiregar/code). 


## My Gratitude {-}

Thank you to fellow academics and econometric practitioners who have shared their knowledge and insights, provided valuable input, and provided inspiration in preparing this material. I would also like to thank my family and friends who provided moral support and encouragement during the writing process. You are a source of strength and inspiration for me. The entire process of writing this was an incredible journey, and I hope this ebook will be of benefit to readers, both those who are studying econometrics for the first time and those who want to improve their understanding.

Finally, thank you to the publishing team who has worked hard in the production process of this ebook. Hopefully this ebook will be a useful reference source and can contribute to the development of econometric studies.

## Feedback & Suggestions {-}

All your input and feedback means a lot to us to improve this template in the future. For readers/users who wish to submit input and responses, please use the contact below!

**Email:** dsciencelabs@outlook.com


<!--chapter:end:index.Rmd-->

# Introduction to Econometrics

## What is Econometrics?

Econometrics is a branch of economics that combines economic concepts, mathematics, and statistics to analyze and measure empirical relationships in the economy. The term "econometrics" comes from combining the words "economics" and "metrics" (which means measurement). The main goal of econometrics is to develop mathematical models that can be used to explain and predict economic behavior, as well as test economic hypotheses and policies.

```{r econometrics, echo=FALSE,fig.align='center', out.width = '100%'}
knitr::include_graphics("images/Bab1/econometrics.png")
```

In practice, econometrics involves applying statistical models to examine relationships between economic variables. By using empirical data, econometricians (econometric practitioners) try to formulate mathematical models that can explain or predict economic behavior in the real world. The main method in econometrics involves statistical regression, in which economic variables are explained and related to each other.

Some basic concepts in econometrics involve:

* **Regression Model:** Create a mathematical model that describes the relationship between the dependent variable and the independent variable.
* **Hypothesis and Statistical Test:** Develop and test hypotheses about relationships between variables, and determine the extent to which analysis results are statistically reliable.
* **Prediction:** Uses an econometric model to predict the value of the dependent variable based on the value of the given independent variable.
* **Economic Policy Evaluation:** Analyze the effects of economic policies using econometric models.

Econometrics has an important role in economic and policy analysis. This allows economists to use empirical data and statistical methods to make more informational and evidence-based decisions in an economic context.

## Goals of Econometrics

The main goal of econometrics is to develop mathematical models that can describe and explain the relationships between economic variables, as well as test economic hypotheses using empirical data. Some specific goals of econometrics involve:

* **Description and Measurement of Economic Relationships:** Econometrics aims to describe and measure the relationships between economic variables. This involves developing a mathematical model that reflects these relationships.
* **Economic Hypothesis Testing:** Econometrics is used to test economic hypotheses, such as whether there is a causal relationship between two variables or whether the effects of an economic policy are significant.
* **Forecasting and Prediction:** Econometrics provides tools for making forecasts and predictions about future economic behavior. By using the developed model, econometricians can estimate the value of the dependent variable based on the value of the given independent variable.
* **Economic Policy Evaluation:** Econometrics is used to evaluate the impact of economic policies. This allows economists to understand how changes in certain economic variables can affect other variables, thereby helping in policy decision making.
* **Understanding of Economic Processes:** Through statistical analysis, econometrics can help in deep understanding of complex economic processes. This includes identifying factors that influence the behavior of consumers, producers, and the market as a whole.
* **Empirical Research:** Econometrics provides tools for economic researchers to test economic theories using empirical data. This can help fill the gap between economic theory and the economic reality that occurs on the ground.
* **Evidence-Based Decision Making:** One of the main goals of econometrics is to provide an empirical basis for decision making in an economic context. By using data and statistical methods, econometrics helps make more informed and evidence-based decisions.

Thus, econometrics has an important role in providing powerful analytical tools for economists to understand and interpret economic phenomena in the real world.


## Econometric Relations and Regression

Regression is a very important analytical tool in econometrics, helping researchers and analysts to measure and understand economic relationships based on empirical data.

* **Use of Regression in Econometrics:** Regression is one of the main tools in econometric analysis. Econometricians use regression techniques to estimate parameters and measure relationships between economic variables.
* **Regression Models in Econometrics:** In the context of econometrics, regression models are often used to describe and understand causal relationships between economic variables. This model can include economic variables such as income, prices, production, and others.
* **Hypothesis Testing in Regression Context:** Econometricians use regression to test economic hypotheses. For example, does an increase in variable A cause an increase or decrease in variable B.
* **Prediction and Forecasting:** Regression analysis is used in econometrics to make predictions and forecasting. By using a regression model, econometricians can estimate the value of the dependent variable based on the value of the independent variable.

```{r regression, echo=FALSE,fig.align='center', out.width = '100%'}
knitr::include_graphics("images/Bab1/regression.png")
```

## Lab Practicum

### Simple Regression

Suppose you have data on product sales (Y) and advertising costs (X) for several months. Perform a simple regression to understand the relationship between advertising costs and sales. Use the following dataset:

```{r}
# Data
sales <- c(100, 120, 130, 140, 160)
advert <- c(50, 70, 90, 110, 130)

# Simple Regression Model
model1 <- lm(sales ~ advert)

# results
summary(model1)
```

Based on the regression results above, interpret the regression coefficient, R-squared value, and significance test to see whether advertising costs have a significant influence on sales.


### Multiple Regression

Suppose you have a production dataset that includes production (Y), number of hours worked (X1), and number of machines (X2). Perform multiple regression to model the production relationship with these independent variables.


```{r}
# Data
product <- c(200, 250, 300, 280, 320)
work_hours <- c(40, 50, 60, 55, 65)
number_of_machines <- c(2, 3, 4, 3, 5)

# Multiple Regression model
model2 <- lm(product  ~ work_hours + number_of_machines)

# result
summary(model2)
```

Berdasarkan hasil regresi berganda di atas, identifikasi variabel yang memiliki pengaruh signifikan terhadap produksi. Interpretasikan koefisien regresi dan uji signifikan masing-masing variabel.

### Catatan{-}

* Pastikan untuk memberikan penjelasan yang jelas tentang interpretasi hasil, termasuk signifikansi statistik dan koefisien determinasi (R-squared).
* Anda dapat menambahkan visualisasi grafik, seperti plot regresi atau residu, untuk memperjelas analisis.
* Selalu periksa dan pastikan bahwa dataset yang digunakan sesuai dengan konteks soal dan telah diimport dengan benar ke dalam lingkungan R.

<!--chapter:end:01_Intro_to_Econometrics.Rmd-->

# Simple Regression

## What is Regression?

Regression is a set of statistical methods used for the estimation of relationships between a dependent variable and one or more independent variables. It can be utilized to assess the strength of the relationship between variables by fitting a line to the observed data. Regression allows you to estimate how the dependent variable changes as the independent variable(s) change. There are several variations of Regression, such as linear, multiple linear, and nonlinear.


The most common models are simple linear and multiple linear. Nonlinear regression analysis is commonly used for more complicated data sets in which the dependent and independent variables show a nonlinear relationship.


## Measures Of The Relationship 

There are two quantitative measures of such relationships:

### Covariance

In probability theory and statistics, covariance is a measure of the joint variability of two random variables. 

$$
\begin{aligned}
cov(x,y)={\sum^n_{i=1} (x_i-\bar{x})(y_i-\bar{y}) \over n-1} \\
\end{aligned}
$$

Covariance between two variables:

* $cov(x,y) > 0$, $x$ and $y$ tend to move in the same direction
* $cov(x,y) < 0$, $x$ and $y$ tend to move in opposite directions
* $cov(x,y) = 0$, $x$ and $y$ are independent

### Correlation 

The linear correlation coefficient is also referred to as Pearson’s product moment correlation coefficient in honor of Karl Pearson, who originally developed it. This statistic numerically describes how strong the straight-line or linear relationship is between the two variables and the direction, positive or negative.

We can describe the relationship between these two variables graphically and numerically. In order to measure the strength of a linear relationship between two quantitative variables graphically, we use scatter plots:

<br>

```{r correlation, echo=FALSE,fig.align='center', out.width = '100%'}
knitr::include_graphics("images/bab2/correlation.webp")
```

<br>

If you want to measure the strength of a linear relationship between two quantitative variables numerically, use the following form:

$$
\begin{aligned}
r={cov(x,y)\over s_x s_y}
\end{aligned}
$$

where $s_x$ are the sample mean and sample standard deviation of the $x’s$, and $s_y$ are the mean and standard deviation of the $y’s.$

The properties of “r”:

* It is always between -1 and +1.
* It is a unitless measure so “r” would be the same value whether you measured the two variables in pounds and inches or in grams and centimeters.
* Positive values of “r” are associated with positive relationships.
* Negative values of “r” are associated with negative relationships.

## The General Model

A simple linear regression model assumes that a linear relationship exists between the conditional expectation of two quantitative variables:

```{r linear, echo=FALSE,fig.align='center', out.width = '80%'}
knitr::include_graphics("images/Bab2/linear.webp")
```

where,

* $Y$ is regarded as the predictor, explanatory, or dependent variable.
* $X$ is regarded as the response, outcome, or independent variable.
* $\beta_1$  is the intercept parameter or coefficient
* $\beta_2$  is the slope parameter or coefficient
* $\hat{Y}$ is the estimated value of $Y$ given $X$
* $b_1$ and $b_2$ is the coefficient estimated value 


### SLR: The Mathematical Model

The mathematical representation of a simple linear regression model a model with one dependent variable and only one independent variable can be expressed as: 

$$
\begin{aligned}
E(y|x)=\mu_y|x=\beta_1+β_2x
\end{aligned}
$$

where $\beta_1$ and $\beta_2$ are the regression parameters that represent the intercept and slope, respectively. We can interpret the above equation as, “The expected value of the depepndent variable $y$ given $x$ is equal to the mean of $y$ given $x.$” The expected value of $y$ given $x,$ according to the equation 3, can be mathematically represented with an intercept $\beta_1$ and a coefficient of $x, \beta_2.$ The conditional mean $E(y|x)$ is called a simple linear regression model because it contains only one non-random independent variable $x.$


### SLR: The Statistical Model

The simple linear regression model relies on a few assumptions. First, if $var(y|x)=\sigma^2,$ then the data is said to be homoscedastic. This means that for any value of $x,$ the variance of $y$ remains the same. However, this is an unrealistic assumption in real life. Most “real world data” that you would be working with will be heteroscedastic, which means that the variance does not remain the same for a given value of $x.$ The issue of heteroscedasticity is not such a grave sin, depending on the objective and if you have a large enough sample size. However, overcoming this issue is not within the scope of this tutorial. It is just one of the assumptions of the simple linear regression model.

The five assumptions of the simple linear regression model are:

* The mean value of $y$ for each value of $x$ is given by the regression function $E(y|x)=\mu y|x=\beta_1+\beta_2x.$
* For each value of $x,$ the values of $y$ are distributed about their mean value, following the probability distribution that all have the same variance (i.e. homoscedasticity). That is, $var(y|x)=\sigma^2.$
* The sample values of $y$ are all uncorrelated and have zero covariance, which implies that no linear association exists between them. That is, $cov(y_i,y_j)=0.$
* The value $x$ is not random and must take at least two values.
* (Optional) The values of $y$ follow a Gaussian Distribution (i.e. normally distributed about their mean for each value of $x$). This means that $y∼N[\beta_1+\beta_2x,\sigma^2]$


### The Least Squares Principle

The goal in simple linear regression is to estimate the parameters (the $\beta’s$). To do so, we must first introduce the least squares principle, the method we will use to estimate these parameters. The **least squares principle** states that the “line of best fit” is the one that minimizes the distance between each point $(x,y)$ to the regression line. In terms of notation, the parameter estimates of $\beta_1$ and $\beta_2$ are generally denoted as $_1$ and $b_2$, respectively.

The equation for the line of best fit is given by

$$
\begin{aligned}
\hat{y}_i=b_1+b_2x_i
\end{aligned}
$$

The vertical distances between the line of best fit and each point $(x,y)$ are referred to as the **least squares residuals**, which is given by

$$
\begin{aligned}
\hat{e}_i=y_i−\hat{y}_i=y_i−b_1+b_2x_i
\end{aligned}
$$

In order to find the values for $b_1$ and $b_2,$ we want to find the values for the unknown parameters $\beta_1$ and $\beta_2$ that minimizes the sum of squares function

$$
\begin{aligned}
S(\beta_1,\beta_2)=\sum_{i=1}^N e^2_i= \biggl(y_i−\beta_1+\beta_2x_i \biggr) ^2
\end{aligned}
$$

The **least squares estimator** $b_1$ and $b_2$ can be solved using the following equations

$$
\begin{aligned}
b_2 & = {\sum^N_{i=1}(x_i−\bar{x})(y_i−\bar{y}) \over \sum^N_{i=1}(x_i−\bar{x})^2 }\\
b_1& = \bar{y}−b_2\bar{x}
\end{aligned}
$$

### Least Squares Estimators

It is important to note that we may never know the true value of $\beta_1$ and $\beta_2,$ and consequently may never know whether our estimates $b_1$ and $b_2$ are close to the actual values. The least squares estimates $\hat{y_i}$ are random variables because they depend on the random variable $y.$ However, if all of our assumptions hold, then $E(b_2)=\beta_2$. In a situation where the estimators are equal to their true parameter values, then they are said to be unbiased estimators. If we have a large sample size $N$ and all of the assumptions for the simple linear regression model hold, then the value of the estimators $b_1$ and $b_2$ obtained from all the samples will be $\beta_1$ and $\beta_2,$ respectively. The unbiasedness property depends on having many samples of data from the same population.

If all of the assuptions of the linear regression model hold, then the variances and covariances of the estimators are

$$
\begin{aligned}
var(b_1) & =\sigma^2 \biggl[{\sum^n_{i=1}x^2_i \over N(x_i−\bar{x})^2}\biggl] \\
var(b_2) & ={\sigma^2 \over \sum(x_i−\bar{x})^2}  \\
cov(b_1,b_2) & =\sigma^2 \biggl[{−\bar{x} \over ∑^n_{i=1}(x_i−\bar{x})^2}\biggr] \\
\end{aligned}
$$

The variance of an estimator measures the precision of the estimator in the sense that it tells us how much the estimates can vary from sample to sample. ***The smaller the variance, the greater the sampling precision***. Then the Gauss-Markov Theorem states that the estimators $b_1$ and $b_2$ have the smallest variance of all linear and unbiased estimators of $\beta_1$ and $\beta_2.$ Such estimators are commonly referred to as the best linear unbiased estimator (BLUE).


### Estimate the Var and Cov 

Recall that the variance of the random error term $e$ is

$$
\begin{aligned}
var(e_i)=E\biggl[e_i−E(e_i)\biggr]^2
\end{aligned}
$$

If the assumption $E(e_i)=0.$ If the assumption holds, we can then rewrite the variance of the random error term $e$ as

$$
\begin{aligned}
var(e_i)=\sigma^2=E[e_i]^2={1\over N} \sum_{i=1}^N e^2_i
\end{aligned}
$$

Unfortunately, the above formula is of little to no use since the random error term $e_i$ is unobservable. Fortunately, since $e_i=y_i−E(y|x)=y_i−\beta1+\beta_2x_i,$ we can replace the unknown parameters $\beta_1$ and $\beta_2$ with their least squares estimates $b_1$ and $b_2$ to obtain

$$
\begin{aligned}
\hat{e}_i=y_i−\hat{y}_i=y_i−b_1+b_2x_i
\end{aligned}
$$

The unbiased estimator of the variance of the random error term is

$$
\begin{aligned}
\sigma^2={\sum \hat{e}_i \over N−2}
\end{aligned}
$$

where 2 represents the number of parameters (in this case $\beta_1$ and $\beta_2$). We refer to this as having $N−2$ degrees of freedom. Now that we have an unbiased estimator of the variance of our random error term $e,$ we can estimate the variances and covarianes of their least squares estimates.

$$
\begin{aligned}
\widehat{var(b_1)} & = \hat{\sigma^2} \biggl[{\sum^n_{i=1}x^2_i \over N(x_i−\bar{x})^2}\biggl] \\
\widehat{var(b_2)} & ={\hat{\sigma^2} \over \sum(x_i−\bar{x})^2}  \\
\widehat{cov(b_1,b_2)} & = \hat{\sigma^2} \biggl[{−\bar{x} \over ∑^n_{i=1}(x_i−\bar{x})^2}\biggr] \\
\end{aligned}
$$


## Lab Practicum

The data for this example is stored in the R package `PoEdata`. First, you have to install the "PoEdata" package, type the following script lines in the Console window of RStudio:


```{r library, eval=FALSE}
# install.packages("remotes")
# remotes::install_github("ccolonescu/PoEdata")
```

Then, consider the data set:

```{r, message=FALSE}
library(PoEdata)                                     # load the library PoEdata
library(DT)                                          # use for datatable fuction 
data("food", package="PoEdata")
datatable(food)
```

It is always a good idea to visually inspect the data in a scatter diagram, which can be created using the function `plot()`. The figure bellow is a scatter diagram of food expenditure on income, suggesting that there is a positive relationship between income and food expenditure.


```{r}
plot(food$income, food$food_exp, 
     ylim=c(0, max(food$food_exp)),
     xlim=c(0, max(food$income)),
     xlab="weekly income in $100", 
     ylab="weekly food expenditure in $", 
     type = "p")
```

[About $R^2$](https://www.investopedia.com/terms/r/r-squared.asp)

### Estimating a Linear Regression

For the food expenditure data, the regression model will be

$$
\begin{aligned}
Y&=\beta_1+\beta_2 *X+e \\
food_{exp}&=\beta_1+\beta_2*income+e
\end{aligned}
$$

The R function for estimating a linear regression model is `lm(y~x, data)` which, used just by itself does not show any output; It is useful to give the model a name, such as `mod1`, then show the results using `summary(mod1)`. 

```{r}
mod1 <- lm(food_exp ~ income, data = food)           # linear model
smod1<- summary(mod1)                                # get summary 
smod1
```

If you are interested in only some of the results of the regression, such as the estimated coefficients, you can retrieve them using specific functions, such as the function `coef()`.

```{r}
b1 <- coef(mod1)[[1]]                                # the estimated value of  β1
b2 <- coef(mod1)[[2]]                                # the estimated value of  β2
b1
b2
```

From the result, we can write our estimating linear regression as:

$$
\begin{aligned}
\hat{Y}&=b_1+b_2*X \\
\hat{food_{exp}}&=83.416+10.20964*income
\end{aligned}
$$

The intercept parameter, $\beta_1$ , is usually of little importance in econometric models; we are mostly interested in the slope parameter,  $\beta_2$.  The estimated value of  $\beta_2$  suggests that the food expenditure for an average family increases by 10.209643 when the family income increases by 1 unit, which in this case is \$100. The  R  function `abline()` adds the `regfression` line to the previously plotted scatter diagram bellow:

```{r}
plot(food$income, food$food_exp, 
     xlab="weekly income in $100", 
     ylab="weekly food expenditure in $", 
     type = "p")
abline(b1,b2)
```

How can one retrieve various regression results? To retrieve a particular result you just refer to it with the name of the object, followed by the \$ sign and the name of the result you wish to retrieve. For instance, if we want the vector of coefficients from `mod1`, we refer to it as `mod1\$coefficients` and `smod1\$coefficients`:

```{r}
mod1$coefficients
```

```{r}
smod1$coefficients
```

```{r,eval=F}
names(mod1)                                          # for more
names(smod1)                                         # for more
```


As we have seen before, however, some of these results can be retrieved using specific functions, such as `coef(mod1), resid(mod1), fitted(mod1), and vcov(mod1)`.

***

### Prediction with SLR Model

The estimated regression parameters,  $b_1$  and  $b_2$  allow us to predict the expected food expenditure for any given income. All we need to do is to plug the estimated parameter values and the given income into an equation like Equation  15 . For example, the expected value of  `food_exp`  for an income of \$2000 is calculated in Equation  16 . (Remember to divide the income by 100, since the data for the variable  income  is in hundreds of dollars.)


$$
\begin{aligned}
\hat{food_{exp}}&=83.416+10.20964*20\\
& = $ 287.608861
\end{aligned}
$$

R, however, does this calculations for us with its function called `predict()`. Let us extend slightly the example to more than one income for which we predict food expenditure, say income = \$2000, \$2500, and \$2700. The function `predict()` in R requires that the new values of the independent variables be organized under a particular form, called a data frame. Even when we only want to predict for one income, we need the same data-frame structure. In R, a set of numbers is held together using the structure `c()`. The following sequence shows this example.

```{r}
mod1 <- lm(food_exp~income, data=food)
newx <- data.frame(income = c(20, 25, 27))
yhat <- predict(mod1, newx)
yhat                                                 # prints the result
```

***

### Assess Regression Coefficients

The regression coefficients  $b_1$  and  $b_2$  are random variables, because they depend on sample. In this case, we repeated samples to assess regression coefficients. Let us construct a number of random subsamples from the food data and re-calculate  $b_1$  and  $b_2$ . A random subsample can be constructed using the function `sample()`, as the following example illustrates only for  $b_2$.

```{r}
N <- nrow(food)   # returns the number of observations in the dataset
C <- 50           # desired number of subsamples
S <- 38           # desired sample size

sumb2 <- 0
for (i in 1:C){   # a loop over the number of subsamples
  set.seed(3*i)   # a different seed for each subsample  
  subsample <- food[sample(1:N, size=S, replace=TRUE), ]
  mod2 <- lm(food_exp~income, data=subsample)
  #sum b2 for all subsamples:
  sumb2 <- sumb2 + coef(mod2)[[2]]
}
print(sumb2/C, digits = 3)
```

The result,  $b_2=9.88$, is the average of 50 estimates of  $b_2$.

***

### Estimated Var and Cov

Many applications require estimates of the variances and covariances of the regression coefficients. R stores them in the a matrix `vcov()`:

```{r}
(varb1 <- vcov(mod1)[1, 1])
(varb2 <- vcov(mod1)[2, 2])
(covb1b2 <- vcov(mod1)[1,2])
```

### Non-Linear Relationships

Sometimes the scatter plot diagram or some theoretical consideraions suggest a non-linear relationship. The most popular non-linear relationships involve logarithms of the dependent or independent variables and polinomial functions. The quadratic model requires the square of the independent variable.


$$
\begin{aligned}
Y=\beta_1+\beta_2*X^2+e\\
\end{aligned}
$$
In R, independent variables involving mathematical operators can be included in a regression equation with the function `I()`. The following example uses the dataset br from the package `PoEdata`, which includes the sale prices and the attributes of 1080 houses in Baton Rouge, LA. price is the sale price in dollars, and sqft is the surface area in square feet.

```{r}
data(br)
mod2 <- lm(price~I(sqft^2), data=br)
b1 <- coef(mod2)[[1]]
b2 <- coef(mod2)[[2]]
sqftx=c(2000, 4000, 6000)            # given values for sqft
pricex=b1+b2*sqftx^2                 # prices corresponding to given sqft 
DpriceDsqft <- 2*b2*sqftx            # marginal effect of sqft on price
elasticity=DpriceDsqft*sqftx/pricex 
b1; b2; DpriceDsqft; elasticity      # prints results
```

We woud like now to draw a scatter diagram and see how the quadratic function fits the data. The next chunk of code provides two alternatives for constructing such a graph. The first simply draws the quadratic function on the scatter diagram, using the R function `curve()`; the second uses the function lines, which requires ordering the data set in increasing values of `sqft` before the regression model is evaluated, such that the resulting fitted values will also come out in the same order.

```{r}
plot(br$sqft, br$price, xlab="Total square feet", 
     ylab="Sale price, $", col="grey")
#add the quadratic curve to the scatter plot:
curve(b1+b2*x^2, col="red", add=TRUE) 
```

An alternative way to draw the fitted curve:


```{r, eval=F}
ordat <- br[order(br$sqft), ] #sorts the dataset after `sqft`
mod2 <- lm(price~I(sqft^2), data=ordat)
plot(br$sqft, br$price, 
     main="Dataset ordered after 'sqft' ", 
     xlab="Total square feet", 
     ylab="Sale price, $", col="grey")
lines(fitted(mod31)~ordat$sqft, col="red")
```

The log-linear model regresses the log of the dependent variable on a linear expression of the independent variable (unless otherwise specified, the log notation stands for natural logarithm, following a usual convention in economics):

$$
\begin{aligned}
Log(Y)=\beta_1+\beta_2*X+e\\
\end{aligned}
$$

One of the reasons to use the log of an independent variable is to make its distribution closer to the normal distribution. Let us draw the histograms of price and log(price) to compare them

```{r}
hist(br$price, col='grey')
hist(log(br$price), col='grey')
```

We are interested, as before, in the estimates of the coefficients and their interpretation, in the fitted values of price, and in the marginal effect of an increase in sqft on price.

```{r}
mod3 <- lm(log(price)~sqft, data=br)
mod3$coefficients
```


The coefficients are  $b_1=   10.84$  and  $b_2=   0.00041$, showing that an increase in the surface area (sqft) of an apartment by one unit (1 sqft) increases the price of the apartment by  0.041  percent. Thus, for a house price of \$100,000, an increase of 100 sqft will increase the price by approximately  100∗0.041  percent, which is equal to \$4112.7. In general, the marginal effect of an increase in $X$  on  $Y$  in Equation  19  is


$$
\begin{aligned}
{dy\over dx}=\beta_2 * Y
\end{aligned}
$$

and the elasticity is


$$
\begin{aligned}
e= {dy\over dx} {X\over Y}=\beta_2 * X
\end{aligned}
$$

The next lines of code show how to draw the fitted values curve of the loglinear model and how to calculate the marginal effect and the elasticity for the median price in the dataset. The fitted values are here calculated using the formula

$$
\begin{aligned}
\hat{Y}= e^{b_1+b_2*X}
\end{aligned}
$$

```{r}
ordat <- br[order(br$sqft), ] #order the dataset
mod4 <- lm(log(price)~sqft, data=ordat)
plot(br$sqft, br$price, col="grey")
lines(exp(fitted(mod4))~ordat$sqft, 
      col="blue", main="Log-linear Model")
```

```{r}
pricex<- median(br$price)
sqftx <- (log(pricex)-coef(mod4)[[1]])/coef(mod4)[[2]]
(DyDx <- pricex*coef(mod4)[[2]])
```

```{r}
(elasticity <- sqftx*coef(mod4)[[2]])
```

R allows us to calculate the same quantities for several (sqft, price) pairs at a time, as shown in the following sequence:


```{r}
b1 <- coef(mod4)[[1]]
b2 <- coef(mod4)[[2]]
#pick a few values for sqft:
sqftx <- c(2000, 3000, 4000) 
#estimate prices for those and add one more:
pricex <- c(100000, exp(b1+b2*sqftx)) 
#re-calculate sqft for all prices:
sqftx <- (log(pricex)-b1)/b2 
#calculate and print elasticities:
(elasticities <- b2*sqftx) 
```
***

### Indicator Variables

An indicator, or binary variable marks the presence or the absence of some attribute of the observational unit, such as gender or race if the observational unit is an individual, or location if the observational unit is a house. In the dataset `utown`, the variable `utown` is  1  if a house is close to the university and  0  otherwise. Here is a simple linear regression model that involves the variable u`town`:

$$
\begin{align}
price_i= \beta_1+\beta_2*utwon_i
\end{align}
$$

The coefficient of such a variable in a simple linear model is equal to the difference between the average prices of the two categories; the intercept coefficient of the model in Equation 22 is equal to the average price of the houses that are not close to university. Let us first calculate the average prices for each category, wich are denoted in the following sequence of code `price0bar` and `price1bar`:


```{r}
data(utown)
price0bar <- mean(utown$price[which(utown$utown==0)])
price1bar <- mean(utown$price[which(utown$utown==1)])
```

The results are: $\bar{price}=$ `r price1bar` close to university, and  $\bar{price}=$ `r price0bar`for those not close. I now show that the same results yield the coefficients of the regression model in Equation 22:

```{r}
mod4 <- lm(price~utown, data=utown)
b1 <- coef(mod4)[[1]] 
b2 <- coef(mod4)[[2]]
b1
b2
```

The results are:  $\bar{price}=b_1=$ `r b1` for non-university houses, and  $\bar{price}=b_1+b_2=$ `r b1+b2` for university houses.


### Monte Carlo Simulation

A Monte Carlo simulation generates random values for the dependent variable when the regression coefficients and the distribution of the random term are given. The following example seeks to determine the distribution of the independent variable in the food expenditure model in Equation 2,

```{r}
N <- 40
x1 <- 10
x2 <- 20
b1 <- 100
b2 <- 10
mu <- 0
sig2e <- 2500
sde <- sqrt(sig2e)
yhat1 <- b1+b2*x1
yhat2 <- b1+b2*x2
curve(dnorm(x, mean=yhat1, sd=sde), 0, 500, col="blue")
curve(dnorm(x, yhat2, sde), 0,500, add=TRUE, col="red")
abline(v=yhat1, col="blue", lty=2)
abline(v=yhat2, col="red", lty=2)
legend("topright", legend=c("f(y|x=10)", 
                            "f(y|x=20)"), lty=1,
       col=c("blue", "red"))
```

Next, we calculate the variance of  $b_2$  and plot the corresponding density function.

$$
\begin{align}
var(b_2)={\sigma^2 \over \sum(x_i−\bar{x})}
\end{align}
$$

```{r}
x <- c(rep(x1, N/2), rep(x2,N/2))
xbar <- mean(x)
sumx2 <- sum((x-xbar)^2)
varb2 <- sig2e/sumx2
sdb2 <- sqrt(varb2)
leftlim <- b2-3*sdb2
rightlim <- b2+3*sdb2
curve(dnorm(x, mean=b2, sd=sdb2), leftlim, rightlim)
abline(v=b2, lty=2)
```

Now, with the same values of  $b_1$ ,  $b_2$ , and error standard deviation, we can generate a set of values for  $Y$ , regress  $Y$  on  $X$ , and calculate an estimated values for the coefficient  $b_2$  and its standard error.

```{r}
set.seed(12345)
y <- b1+b2*x+rnorm(N, mean=0, sd=sde)
mod6 <- lm(y~x)
b1hat <- coef(mod6)[[1]]
b2hat <- coef(mod6)[[2]]
mod6summary <- summary(mod6) #the summary contains the standard errors
seb2hat <- coef(mod6summary)[2,2]
```

The results are  $b_2=11.64$ and  $s_e(b_2)=1.64$. The strength of a Monte Carlo simulation is, however, the possibility of repeating the estimation of the regression parameters for a large number of automatically generated samples. Thus, we can obtain a large number of values for a parameter, say  $b_2$ , and then determine its sampling characteristics. For instance, if the mean of these values is close to the initially asumed value  $b_2=  10$, we conclude that our estimator (the method of estimating the parameter) is unbiased.


We are going to use this time the values of  $X$  in the food dataset, and generate  $Y$  using the linear model with  $b_1=100$ and  $b_2= 10.$

```{r}
data("food")
N <- 40
sde <- 50
x <- food$income
nrsim <- 1000
b1 <- 100
b2 <- 10
vb2 <- numeric(nrsim) #stores the estimates of b2
for (i in 1:nrsim){
  set.seed(12345+10*i)
  y <- b1+b2*x+rnorm(N, mean=0, sd=sde)
  mod7 <- lm(y~x)
  vb2[i] <- coef(mod7)[[2]]
}
mb2 <- mean(vb2)
seb2 <- sd(vb2)
```

The mean and standard deviation of the estimated 40 values of  $b_2$  are, respectively, 9.974985 and 1.152632. Figure bellow shows the simulated distribution of  $b_2$  and the theoretical one.


```{r}
plot(density(vb2))
curve(dnorm(x, mb2, seb2), col="red", add=TRUE)
legend("topright", legend=c("true", "simulated"), 
       lty=1, col=c("red", "black"))
hist(vb2, prob=TRUE, ylim=c(0,.4))
curve(dnorm(x, mean=mb2, sd=seb2), col="red", add=TRUE)
```

```{r}
rm(list=ls()) # Caution: this clears the Environment
```

<!--chapter:end:02_Simple_Regression.Rmd-->

# Referensi


<!--chapter:end:Referensi.Rmd-->

